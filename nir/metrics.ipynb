{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 88, in __main__.average_precision\n",
      "Failed example:\n",
      "    average_precision(r)\n",
      "Expected:\n",
      "    0.78333333333333333\n",
      "    Args:\n",
      "        r: Relevance scores (list or numpy) in rank order\n",
      "            (first element is the first item)\n",
      "    Returns:\n",
      "        Average precision\n",
      "Got:\n",
      "    0.78333333333333333\n",
      "**********************************************************************\n",
      "File \"__main__\", line 138, in __main__.dcg_at_k\n",
      "Failed example:\n",
      "    dcg_at_k(r, 11)\n",
      "Expected:\n",
      "    9.6051177391888114\n",
      "    Args:\n",
      "        r: Relevance scores (list or numpy) in rank order\n",
      "            (first element is the first item)\n",
      "        k: Number of results to consider\n",
      "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
      "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
      "    Returns:\n",
      "        Discounted cumulative gain\n",
      "Got:\n",
      "    9.6051177391888114\n",
      "**********************************************************************\n",
      "File \"__main__\", line 110, in __main__.mean_average_precision\n",
      "Failed example:\n",
      "    mean_average_precision(rs)\n",
      "Expected:\n",
      "    0.39166666666666666\n",
      "    Args:\n",
      "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
      "            (first element is the first item)\n",
      "    Returns:\n",
      "        Mean average precision\n",
      "Got:\n",
      "    0.39166666666666666\n",
      "**********************************************************************\n",
      "File \"__main__\", line 15, in __main__.mean_reciprocal_rank\n",
      "Failed example:\n",
      "    mean_reciprocal_rank(rs)\n",
      "Expected:\n",
      "    0.75\n",
      "    Args:\n",
      "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
      "            (first element is the first item)\n",
      "    Returns:\n",
      "        Mean reciprocal rank\n",
      "Got:\n",
      "    0.75\n",
      "**********************************************************************\n",
      "File \"__main__\", line 176, in __main__.ndcg_at_k\n",
      "Failed example:\n",
      "    ndcg_at_k([1], 2)\n",
      "Expected:\n",
      "    1.0\n",
      "    Args:\n",
      "        r: Relevance scores (list or numpy) in rank order\n",
      "            (first element is the first item)\n",
      "        k: Number of results to consider\n",
      "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
      "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
      "    Returns:\n",
      "        Normalized discounted cumulative gain\n",
      "Got:\n",
      "    1.0\n",
      "**********************************************************************\n",
      "File \"__main__\", line 62, in __main__.precision_at_k\n",
      "Failed example:\n",
      "    precision_at_k(r, 4)\n",
      "Expected:\n",
      "    Traceback (most recent call last):\n",
      "        File \"<stdin>\", line 1, in ?\n",
      "    ValueError: Relevance score length < k\n",
      "    Args:\n",
      "        r: Relevance scores (list or numpy) in rank order\n",
      "            (first element is the first item)\n",
      "    Returns:\n",
      "        Precision @ k\n",
      "    Raises:\n",
      "        ValueError: len(r) must be >= k\n",
      "Got:\n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\Anaconda2\\lib\\doctest.py\", line 1315, in __run\n",
      "        compileflags, 1) in test.globs\n",
      "      File \"<doctest __main__.precision_at_k[4]>\", line 1, in <module>\n",
      "        precision_at_k(r, 4)\n",
      "      File \"<ipython-input-1-b4268731c1bb>\", line 77, in precision_at_k\n",
      "        raise ValueError('Relevance score length < k')\n",
      "    ValueError: Relevance score length < k\n",
      "**********************************************************************\n",
      "File \"__main__\", line 37, in __main__.r_precision\n",
      "Failed example:\n",
      "    r_precision(r)\n",
      "Expected:\n",
      "    1.0\n",
      "    Args:\n",
      "        r: Relevance scores (list or numpy) in rank order\n",
      "            (first element is the first item)\n",
      "    Returns:\n",
      "        R Precision\n",
      "Got:\n",
      "    1.0\n",
      "**********************************************************************\n",
      "7 items had failures:\n",
      "   1 of   4 in __main__.average_precision\n",
      "   1 of   7 in __main__.dcg_at_k\n",
      "   1 of   4 in __main__.mean_average_precision\n",
      "   1 of   6 in __main__.mean_reciprocal_rank\n",
      "   1 of   7 in __main__.ndcg_at_k\n",
      "   1 of   5 in __main__.precision_at_k\n",
      "   1 of   6 in __main__.r_precision\n",
      "***Test Failed*** 7 failures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=7, attempted=39)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(rs):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "\n",
    "\n",
    "def r_precision(r):\n",
    "    \"\"\"Score is precision after all relevant documents have been retrieved\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> r_precision(r)\n",
    "    0.33333333333333331\n",
    "    >>> r = [0, 1, 0]\n",
    "    >>> r_precision(r)\n",
    "    0.5\n",
    "    >>> r = [1, 0, 0]\n",
    "    >>> r_precision(r)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        R Precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    z = r.nonzero()[0]\n",
    "    if not z.size:\n",
    "        return 0.\n",
    "    return np.mean(r[:z[-1] + 1])\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        mes = 'Relevance score length' + str(r.size) + ' < k ' + str(k)\n",
    "        raise ValueError(mes)\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.78333333333333333\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.39166666666666666\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
